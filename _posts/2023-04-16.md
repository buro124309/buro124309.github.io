---
jupyter:
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.7.0
  nbformat: 4
  nbformat_minor: 4
---

::: {.cell .markdown}
**As always, I hope you find this kernel useful and your
`<font color="red">`{=html}`<b>`{=html}UPVOTES`</b>`{=html}`</font>`{=html}
would be highly appreciated**.
:::

::: {.cell .markdown}
`<a class="anchor" id="0">`{=html}`</a>`{=html}

# **Logistic Regression Classifier Tutorial with Python**

Hello friends,

In this kernel, I implement Logistic Regression with Python and
Scikit-Learn. I build a Logistic Regression classifier to predict
whether or not it will rain tomorrow in Australia. I train a binary
classification model using Logistic Regression.
:::

::: {.cell .markdown}
`<a class="anchor" id="0.1">`{=html}`</a>`{=html}

# **Table of Contents**

1.  [Introduction to Logistic Regression](#1)
2.  [Logistic Regression intuition](#2)
3.  [Assumptions of Logistic Regression](#3)
4.  [Types of Logistic Regression](#4)
5.  [Import libraries](#5)
6.  [Import dataset](#6)
7.  [Exploratory data analysis](#7)
8.  [Declare feature vector and target variable](#8)
9.  [Split data into separate training and test set](#9)
10. [Feature engineering](#10)
11. [Feature scaling](#11)
12. [Model training](#12)
13. [Predict results](#13)
14. [Check accuracy score](#14)
15. [Confusion matrix](#15)
16. [Classification metrices](#16)
17. [Adjusting the threshold level](#17)
18. [ROC - AUC](#18)
19. [k-Fold Cross Validation](#19)
20. [Hyperparameter optimization using GridSearch CV](#20)
21. [Results and conclusion](#21)
22. [References](#22)
:::

::: {.cell .markdown}
# **1. Introduction to Logistic Regression** `<a class="anchor" id="1">`{=html}`</a>`{=html} {#1-introduction-to-logistic-regression-}

[Table of Contents](#0.1)

When data scientists may come across a new classification problem, the
first algorithm that may come across their mind is **Logistic
Regression**. It is a supervised learning classification algorithm which
is used to predict observations to a discrete set of classes.
Practically, it is used to classify observations into different
categories. Hence, its output is discrete in nature. **Logistic
Regression** is also called **Logit Regression**. It is one of the most
simple, straightforward and versatile classification algorithms which is
used to solve classification problems.
:::

::: {.cell .markdown}
# **2. Logistic Regression intuition** `<a class="anchor" id="2">`{=html}`</a>`{=html} {#2-logistic-regression-intuition-}

[Table of Contents](#0.1)

In statistics, the **Logistic Regression model** is a widely used
statistical model which is primarily used for classification purposes.
It means that given a set of observations, Logistic Regression algorithm
helps us to classify these observations into two or more discrete
classes. So, the target variable is discrete in nature.

The Logistic Regression algorithm works as follows -
:::

::: {.cell .markdown}
## **Implement linear equation**

Logistic Regression algorithm works by implementing a linear equation
with independent or explanatory variables to predict a response value.
For example, we consider the example of number of hours studied and
probability of passing the exam. Here, number of hours studied is the
explanatory variable and it is denoted by x1. Probability of passing the
exam is the response or target variable and it is denoted by z.

If we have one explanatory variable (x1) and one response variable (z),
then the linear equation would be given mathematically with the
following equation-

    z = β0 + β1x1    

Here, the coefficients β0 and β1 are the parameters of the model.

If there are multiple explanatory variables, then the above equation can
be extended to

    z = β0 + β1x1+ β2x2+……..+ βnxn

Here, the coefficients β0, β1, β2 and βn are the parameters of the
model.

So, the predicted response value is given by the above equations and is
denoted by z.
:::

::: {.cell .markdown}
## **Sigmoid Function**

This predicted response value, denoted by z is then converted into a
probability value that lie between 0 and 1. We use the sigmoid function
in order to map predicted values to probability values. This sigmoid
function then maps any real value into a probability value between 0 and
1.

In machine learning, sigmoid function is used to map predictions to
probabilities. The sigmoid function has an S shaped curve. It is also
called sigmoid curve.

A Sigmoid function is a special case of the Logistic function. It is
given by the following mathematical formula.

Graphically, we can represent sigmoid function with the following graph.
:::

::: {.cell .markdown}
### Sigmoid Function {#sigmoid-function}

![Sigmoid
Function](vertopal_6098b8dbd2864528b91b89bdc2279afb/3d802ba40a4e95f9c3e7ba257488dc4da0cd5fe4.png)
:::

::: {.cell .markdown}
## **Decision boundary**

The sigmoid function returns a probability value between 0 and 1. This
probability value is then mapped to a discrete class which is either "0"
or "1". In order to map this probability value to a discrete class
(pass/fail, yes/no, true/false), we select a threshold value. This
threshold value is called Decision boundary. Above this threshold value,
we will map the probability values into class 1 and below which we will
map values into class 0.

Mathematically, it can be expressed as follows:-

p ≥ 0.5 =\> class = 1

p \< 0.5 =\> class = 0

Generally, the decision boundary is set to 0.5. So, if the probability
value is 0.8 (\> 0.5), we will map this observation to class 1.
Similarly, if the probability value is 0.2 (\< 0.5), we will map this
observation to class 0. This is represented in the graph below-
:::

::: {.cell .markdown}
![Decision boundary in sigmoid
function](vertopal_6098b8dbd2864528b91b89bdc2279afb/b7f4443f8c2153c2c7b994d7b8f21de575acf06f.png)
:::

::: {.cell .markdown}
## **Making predictions**

Now, we know about sigmoid function and decision boundary in logistic
regression. We can use our knowledge of sigmoid function and decision
boundary to write a prediction function. A prediction function in
logistic regression returns the probability of the observation being
positive, Yes or True. We call this as class 1 and it is denoted by
P(class = 1). If the probability inches closer to one, then we will be
more confident about our model that the observation is in class 1,
otherwise it is in class 0.
:::

::: {.cell .markdown}
# **3. Assumptions of Logistic Regression** `<a class="anchor" id="3">`{=html}`</a>`{=html} {#3-assumptions-of-logistic-regression-}

[Table of Contents](#0.1)

The Logistic Regression model requires several key assumptions. These
are as follows:-

1.  Logistic Regression model requires the dependent variable to be
    binary, multinomial or ordinal in nature.

2.  It requires the observations to be independent of each other. So,
    the observations should not come from repeated measurements.

3.  Logistic Regression algorithm requires little or no
    multicollinearity among the independent variables. It means that the
    independent variables should not be too highly correlated with each
    other.

4.  Logistic Regression model assumes linearity of independent variables
    and log odds.

5.  The success of Logistic Regression model depends on the sample
    sizes. Typically, it requires a large sample size to achieve the
    high accuracy.
:::

::: {.cell .markdown}
# **4. Types of Logistic Regression** `<a class="anchor" id="4">`{=html}`</a>`{=html} {#4-types-of-logistic-regression-}

[Table of Contents](#0.1)

Logistic Regression model can be classified into three groups based on
the target variable categories. These three groups are described below:-

### 1. Binary Logistic Regression {#1-binary-logistic-regression}

In Binary Logistic Regression, the target variable has two possible
categories. The common examples of categories are yes or no, good or
bad, true or false, spam or no spam and pass or fail.

### 2. Multinomial Logistic Regression {#2-multinomial-logistic-regression}

In Multinomial Logistic Regression, the target variable has three or
more categories which are not in any particular order. So, there are
three or more nominal categories. The examples include the type of
categories of fruits - apple, mango, orange and banana.

### 3. Ordinal Logistic Regression {#3-ordinal-logistic-regression}

In Ordinal Logistic Regression, the target variable has three or more
ordinal categories. So, there is intrinsic order involved with the
categories. For example, the student performance can be categorized as
poor, average, good and excellent.
:::

::: {.cell .markdown}
# **5. Import libraries** `<a class="anchor" id="5">`{=html}`</a>`{=html} {#5-import-libraries-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # data visualization
import seaborn as sns # statistical data visualization
%matplotlib inline

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
```
:::

::: {.cell .code trusted="true"}
``` python
import warnings

warnings.filterwarnings('ignore')
```
:::

::: {.cell .markdown}
# **6. Import dataset** `<a class="anchor" id="6">`{=html}`</a>`{=html} {#6-import-dataset-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
data = '/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv'

df = pd.read_csv(data)
```
:::

::: {.cell .markdown}
# **7. Exploratory data analysis** `<a class="anchor" id="7">`{=html}`</a>`{=html} {#7-exploratory-data-analysis-}

[Table of Contents](#0.1)

Now, I will explore the data to gain insights about the data.
:::

::: {.cell .code trusted="true"}
``` python
# view dimensions of dataset

df.shape
```
:::

::: {.cell .markdown}
We can see that there are 142193 instances and 24 variables in the data
set.
:::

::: {.cell .code trusted="true"}
``` python
# preview the dataset

df.head()
```
:::

::: {.cell .code trusted="true"}
``` python
col_names = df.columns

col_names
```
:::

::: {.cell .markdown}
### Drop RISK_MM variable

It is given in the dataset description, that we should drop the
`RISK_MM` feature variable from the dataset description. So, we should
drop it as follows-
:::

::: {.cell .code trusted="true"}
``` python
df.drop(['RISK_MM'], axis=1, inplace=True)
```
:::

::: {.cell .code trusted="true"}
``` python
# view summary of dataset

df.info()
```
:::

::: {.cell .markdown}
### Types of variables

In this section, I segregate the dataset into categorical and numerical
variables. There are a mixture of categorical and numerical variables in
the dataset. Categorical variables have data type object. Numerical
variables have data type float64.

First of all, I will find categorical variables.
:::

::: {.cell .code trusted="true"}
``` python
# find categorical variables

categorical = [var for var in df.columns if df[var].dtype=='O']

print('There are {} categorical variables\n'.format(len(categorical)))

print('The categorical variables are :', categorical)
```
:::

::: {.cell .code trusted="true"}
``` python
# view the categorical variables

df[categorical].head()
```
:::

::: {.cell .markdown}
### Summary of categorical variables

-   There is a date variable. It is denoted by `Date` column.

-   There are 6 categorical variables. These are given by `Location`,
    `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and
    `RainTomorrow`.

-   There are two binary categorical variables - `RainToday` and
    `RainTomorrow`.

-   `RainTomorrow` is the target variable.
:::

::: {.cell .markdown}
## Explore problems within categorical variables

First, I will explore the categorical variables.

### Missing values in categorical variables
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in categorical variables

df[categorical].isnull().sum()
```
:::

::: {.cell .code trusted="true"}
``` python
# print categorical variables containing missing values

cat1 = [var for var in categorical if df[var].isnull().sum()!=0]

print(df[cat1].isnull().sum())
```
:::

::: {.cell .markdown}
We can see that there are only 4 categorical variables in the dataset
which contains missing values. These are `WindGustDir`, `WindDir9am`,
`WindDir3pm` and `RainToday`.
:::

::: {.cell .markdown}
### Frequency counts of categorical variables

Now, I will check the frequency counts of categorical variables.
:::

::: {.cell .code trusted="true"}
``` python
# view frequency of categorical variables

for var in categorical: 
    
    print(df[var].value_counts())
```
:::

::: {.cell .code trusted="true"}
``` python
# view frequency distribution of categorical variables

for var in categorical: 
    
    print(df[var].value_counts()/np.float(len(df)))
```
:::

::: {.cell .markdown}
### Number of labels: cardinality

The number of labels within a categorical variable is known as
**cardinality**. A high number of labels within a variable is known as
**high cardinality**. High cardinality may pose some serious problems in
the machine learning model. So, I will check for high cardinality.
:::

::: {.cell .code trusted="true"}
``` python
# check for cardinality in categorical variables

for var in categorical:
    
    print(var, ' contains ', len(df[var].unique()), ' labels')
```
:::

::: {.cell .markdown}
We can see that there is a `Date` variable which needs to be
preprocessed. I will do preprocessing in the following section.

All the other variables contain relatively smaller number of variables.
:::

::: {.cell .markdown}
### Feature Engineering of Date Variable
:::

::: {.cell .code trusted="true"}
``` python
df['Date'].dtypes
```
:::

::: {.cell .markdown}
We can see that the data type of `Date` variable is object. I will parse
the date currently coded as object into datetime format.
:::

::: {.cell .code trusted="true"}
``` python
# parse the dates, currently coded as strings, into datetime format

df['Date'] = pd.to_datetime(df['Date'])
```
:::

::: {.cell .code trusted="true"}
``` python
# extract year from date

df['Year'] = df['Date'].dt.year

df['Year'].head()
```
:::

::: {.cell .code trusted="true"}
``` python
# extract month from date

df['Month'] = df['Date'].dt.month

df['Month'].head()
```
:::

::: {.cell .code trusted="true"}
``` python
# extract day from date

df['Day'] = df['Date'].dt.day

df['Day'].head()
```
:::

::: {.cell .code trusted="true"}
``` python
# again view the summary of dataset

df.info()
```
:::

::: {.cell .markdown}
We can see that there are three additional columns created from `Date`
variable. Now, I will drop the original `Date` variable from the
dataset.
:::

::: {.cell .code trusted="true"}
``` python
# drop the original Date variable

df.drop('Date', axis=1, inplace = True)
```
:::

::: {.cell .code trusted="true"}
``` python
# preview the dataset again

df.head()
```
:::

::: {.cell .markdown}
Now, we can see that the `Date` variable has been removed from the
dataset.
:::

::: {.cell .markdown}
### Explore Categorical Variables

Now, I will explore the categorical variables one by one.
:::

::: {.cell .code trusted="true"}
``` python
# find categorical variables

categorical = [var for var in df.columns if df[var].dtype=='O']

print('There are {} categorical variables\n'.format(len(categorical)))

print('The categorical variables are :', categorical)
```
:::

::: {.cell .markdown}
We can see that there are 6 categorical variables in the dataset. The
`Date` variable has been removed. First, I will check missing values in
categorical variables.
:::

::: {.cell .code trusted="true"}
``` python
# check for missing values in categorical variables 

df[categorical].isnull().sum()
```
:::

::: {.cell .markdown}
We can see that `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday`
variables contain missing values. I will explore these variables one by
one.
:::

::: {.cell .markdown}
### Explore `Location` variable
:::

::: {.cell .code trusted="true"}
``` python
# print number of labels in Location variable

print('Location contains', len(df.Location.unique()), 'labels')
```
:::

::: {.cell .code trusted="true"}
``` python
# check labels in location variable

df.Location.unique()
```
:::

::: {.cell .code trusted="true"}
``` python
# check frequency distribution of values in Location variable

df.Location.value_counts()
```
:::

::: {.cell .code trusted="true"}
``` python
# let's do One Hot Encoding of Location variable
# get k-1 dummy variables after One Hot Encoding 
# preview the dataset with head() method

pd.get_dummies(df.Location, drop_first=True).head()
```
:::

::: {.cell .markdown}
### Explore `WindGustDir` variable
:::

::: {.cell .code trusted="true"}
``` python
# print number of labels in WindGustDir variable

print('WindGustDir contains', len(df['WindGustDir'].unique()), 'labels')
```
:::

::: {.cell .code trusted="true"}
``` python
# check labels in WindGustDir variable

df['WindGustDir'].unique()
```
:::

::: {.cell .code trusted="true"}
``` python
# check frequency distribution of values in WindGustDir variable

df.WindGustDir.value_counts()
```
:::

::: {.cell .code trusted="true"}
``` python
# let's do One Hot Encoding of WindGustDir variable
# get k-1 dummy variables after One Hot Encoding 
# also add an additional dummy variable to indicate there was missing data
# preview the dataset with head() method

pd.get_dummies(df.WindGustDir, drop_first=True, dummy_na=True).head()
```
:::

::: {.cell .code trusted="true"}
``` python
# sum the number of 1s per boolean variable over the rows of the dataset
# it will tell us how many observations we have for each category

pd.get_dummies(df.WindGustDir, drop_first=True, dummy_na=True).sum(axis=0)
```
:::

::: {.cell .markdown}
We can see that there are 9330 missing values in WindGustDir variable.
:::

::: {.cell .markdown}
### Explore `WindDir9am` variable
:::

::: {.cell .code trusted="true"}
``` python
# print number of labels in WindDir9am variable

print('WindDir9am contains', len(df['WindDir9am'].unique()), 'labels')
```
:::

::: {.cell .code trusted="true"}
``` python
# check labels in WindDir9am variable

df['WindDir9am'].unique()
```
:::

::: {.cell .code trusted="true"}
``` python
# check frequency distribution of values in WindDir9am variable

df['WindDir9am'].value_counts()
```
:::

::: {.cell .code trusted="true"}
``` python
# let's do One Hot Encoding of WindDir9am variable
# get k-1 dummy variables after One Hot Encoding 
# also add an additional dummy variable to indicate there was missing data
# preview the dataset with head() method

pd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True).head()
```
:::

::: {.cell .code trusted="true"}
``` python
# sum the number of 1s per boolean variable over the rows of the dataset
# it will tell us how many observations we have for each category

pd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True).sum(axis=0)
```
:::

::: {.cell .markdown}
We can see that there are 10013 missing values in the `WindDir9am`
variable.
:::

::: {.cell .markdown}
### Explore `WindDir3pm` variable
:::

::: {.cell .code trusted="true"}
``` python
# print number of labels in WindDir3pm variable

print('WindDir3pm contains', len(df['WindDir3pm'].unique()), 'labels')
```
:::

::: {.cell .code trusted="true"}
``` python
# check labels in WindDir3pm variable

df['WindDir3pm'].unique()
```
:::

::: {.cell .code trusted="true"}
``` python
# check frequency distribution of values in WindDir3pm variable

df['WindDir3pm'].value_counts()
```
:::

::: {.cell .code trusted="true"}
``` python
# let's do One Hot Encoding of WindDir3pm variable
# get k-1 dummy variables after One Hot Encoding 
# also add an additional dummy variable to indicate there was missing data
# preview the dataset with head() method

pd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True).head()
```
:::

::: {.cell .code trusted="true"}
``` python
# sum the number of 1s per boolean variable over the rows of the dataset
# it will tell us how many observations we have for each category

pd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True).sum(axis=0)
```
:::

::: {.cell .markdown}
There are 3778 missing values in the `WindDir3pm` variable.
:::

::: {.cell .markdown}
### Explore `RainToday` variable
:::

::: {.cell .code trusted="true"}
``` python
# print number of labels in RainToday variable

print('RainToday contains', len(df['RainToday'].unique()), 'labels')
```
:::

::: {.cell .code trusted="true"}
``` python
# check labels in WindGustDir variable

df['RainToday'].unique()
```
:::

::: {.cell .code trusted="true"}
``` python
# check frequency distribution of values in WindGustDir variable

df.RainToday.value_counts()
```
:::

::: {.cell .code trusted="true"}
``` python
# let's do One Hot Encoding of RainToday variable
# get k-1 dummy variables after One Hot Encoding 
# also add an additional dummy variable to indicate there was missing data
# preview the dataset with head() method

pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).head()
```
:::

::: {.cell .code trusted="true"}
``` python
# sum the number of 1s per boolean variable over the rows of the dataset
# it will tell us how many observations we have for each category

pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).sum(axis=0)
```
:::

::: {.cell .markdown}
There are 1406 missing values in the `RainToday` variable.
:::

::: {.cell .markdown}
### Explore Numerical Variables
:::

::: {.cell .code trusted="true"}
``` python
# find numerical variables

numerical = [var for var in df.columns if df[var].dtype!='O']

print('There are {} numerical variables\n'.format(len(numerical)))

print('The numerical variables are :', numerical)
```
:::

::: {.cell .code trusted="true"}
``` python
# view the numerical variables

df[numerical].head()
```
:::

::: {.cell .markdown}
### Summary of numerical variables

-   There are 16 numerical variables.

-   These are given by `MinTemp`, `MaxTemp`, `Rainfall`, `Evaporation`,
    `Sunshine`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`,
    `Humidity9am`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`,
    `Cloud9am`, `Cloud3pm`, `Temp9am` and `Temp3pm`.

-   All of the numerical variables are of continuous type.
:::

::: {.cell .markdown}
## Explore problems within numerical variables

Now, I will explore the numerical variables.

### Missing values in numerical variables
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in numerical variables

df[numerical].isnull().sum()
```
:::

::: {.cell .markdown}
We can see that all the 16 numerical variables contain missing values.
:::

::: {.cell .markdown}
### Outliers in numerical variables
:::

::: {.cell .code trusted="true"}
``` python
# view summary statistics in numerical variables

print(round(df[numerical].describe()),2)
```
:::

::: {.cell .markdown}
On closer inspection, we can see that the `Rainfall`, `Evaporation`,
`WindSpeed9am` and `WindSpeed3pm` columns may contain outliers.

I will draw boxplots to visualise outliers in the above variables.
:::

::: {.cell .code trusted="true"}
``` python
# draw boxplots to visualize outliers

plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Rainfall')
fig.set_title('')
fig.set_ylabel('Rainfall')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Evaporation')
fig.set_title('')
fig.set_ylabel('Evaporation')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='WindSpeed9am')
fig.set_title('')
fig.set_ylabel('WindSpeed9am')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='WindSpeed3pm')
fig.set_title('')
fig.set_ylabel('WindSpeed3pm')
```
:::

::: {.cell .markdown}
The above boxplots confirm that there are lot of outliers in these
variables.
:::

::: {.cell .markdown}
### Check the distribution of variables

Now, I will plot the histograms to check distributions to find out if
they are normal or skewed. If the variable follows normal distribution,
then I will do `Extreme Value Analysis` otherwise if they are skewed, I
will find IQR (Interquantile range).
:::

::: {.cell .code trusted="true"}
``` python
# plot histogram to check distribution

plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.Rainfall.hist(bins=10)
fig.set_xlabel('Rainfall')
fig.set_ylabel('RainTomorrow')


plt.subplot(2, 2, 2)
fig = df.Evaporation.hist(bins=10)
fig.set_xlabel('Evaporation')
fig.set_ylabel('RainTomorrow')


plt.subplot(2, 2, 3)
fig = df.WindSpeed9am.hist(bins=10)
fig.set_xlabel('WindSpeed9am')
fig.set_ylabel('RainTomorrow')


plt.subplot(2, 2, 4)
fig = df.WindSpeed3pm.hist(bins=10)
fig.set_xlabel('WindSpeed3pm')
fig.set_ylabel('RainTomorrow')
```
:::

::: {.cell .markdown}
We can see that all the four variables are skewed. So, I will use
interquantile range to find outliers.
:::

::: {.cell .code trusted="true"}
``` python
# find outliers for Rainfall variable

IQR = df.Rainfall.quantile(0.75) - df.Rainfall.quantile(0.25)
Lower_fence = df.Rainfall.quantile(0.25) - (IQR * 3)
Upper_fence = df.Rainfall.quantile(0.75) + (IQR * 3)
print('Rainfall outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))
```
:::

::: {.cell .markdown}
For `Rainfall`, the minimum and maximum values are 0.0 and 371.0. So,
the outliers are values \> 3.2.
:::

::: {.cell .code trusted="true"}
``` python
# find outliers for Evaporation variable

IQR = df.Evaporation.quantile(0.75) - df.Evaporation.quantile(0.25)
Lower_fence = df.Evaporation.quantile(0.25) - (IQR * 3)
Upper_fence = df.Evaporation.quantile(0.75) + (IQR * 3)
print('Evaporation outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))
```
:::

::: {.cell .markdown}
For `Evaporation`, the minimum and maximum values are 0.0 and 145.0. So,
the outliers are values \> 21.8.
:::

::: {.cell .code trusted="true"}
``` python
# find outliers for WindSpeed9am variable

IQR = df.WindSpeed9am.quantile(0.75) - df.WindSpeed9am.quantile(0.25)
Lower_fence = df.WindSpeed9am.quantile(0.25) - (IQR * 3)
Upper_fence = df.WindSpeed9am.quantile(0.75) + (IQR * 3)
print('WindSpeed9am outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))
```
:::

::: {.cell .markdown}
For `WindSpeed9am`, the minimum and maximum values are 0.0 and 130.0.
So, the outliers are values \> 55.0.
:::

::: {.cell .code trusted="true"}
``` python
# find outliers for WindSpeed3pm variable

IQR = df.WindSpeed3pm.quantile(0.75) - df.WindSpeed3pm.quantile(0.25)
Lower_fence = df.WindSpeed3pm.quantile(0.25) - (IQR * 3)
Upper_fence = df.WindSpeed3pm.quantile(0.75) + (IQR * 3)
print('WindSpeed3pm outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))
```
:::

::: {.cell .markdown}
For `WindSpeed3pm`, the minimum and maximum values are 0.0 and 87.0. So,
the outliers are values \> 57.0.
:::

::: {.cell .markdown}
# **8. Declare feature vector and target variable** `<a class="anchor" id="8">`{=html}`</a>`{=html} {#8-declare-feature-vector-and-target-variable-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
X = df.drop(['RainTomorrow'], axis=1)

y = df['RainTomorrow']
```
:::

::: {.cell .markdown}
# **9. Split data into separate training and test set** `<a class="anchor" id="9">`{=html}`</a>`{=html} {#9-split-data-into-separate-training-and-test-set-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
# split X and y into training and testing sets

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
```
:::

::: {.cell .code trusted="true"}
``` python
# check the shape of X_train and X_test

X_train.shape, X_test.shape
```
:::

::: {.cell .markdown}
# **10. Feature Engineering** `<a class="anchor" id="10">`{=html}`</a>`{=html} {#10-feature-engineering-}

[Table of Contents](#0.1)

**Feature Engineering** is the process of transforming raw data into
useful features that help us to understand our model better and increase
its predictive power. I will carry out feature engineering on different
types of variables.

First, I will display the categorical and numerical variables again
separately.
:::

::: {.cell .code trusted="true"}
``` python
# check data types in X_train

X_train.dtypes
```
:::

::: {.cell .code trusted="true"}
``` python
# display categorical variables

categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']

categorical
```
:::

::: {.cell .code trusted="true"}
``` python
# display numerical variables

numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']

numerical
```
:::

::: {.cell .markdown}
### Engineering missing values in numerical variables
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in numerical variables in X_train

X_train[numerical].isnull().sum()
```
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in numerical variables in X_test

X_test[numerical].isnull().sum()
```
:::

::: {.cell .code trusted="true"}
``` python
# print percentage of missing values in the numerical variables in training set

for col in numerical:
    if X_train[col].isnull().mean()>0:
        print(col, round(X_train[col].isnull().mean(),4))
```
:::

::: {.cell .markdown}
### Assumption

I assume that the data are missing completely at random (MCAR). There
are two methods which can be used to impute missing values. One is mean
or median imputation and other one is random sample imputation. When
there are outliers in the dataset, we should use median imputation. So,
I will use median imputation because median imputation is robust to
outliers.

I will impute missing values with the appropriate statistical measures
of the data, in this case median. Imputation should be done over the
training set, and then propagated to the test set. It means that the
statistical measures to be used to fill missing values both in train and
test set, should be extracted from the train set only. This is to avoid
overfitting.
:::

::: {.cell .code trusted="true"}
``` python
# impute missing values in X_train and X_test with respective column median in X_train

for df1 in [X_train, X_test]:
    for col in numerical:
        col_median=X_train[col].median()
        df1[col].fillna(col_median, inplace=True)           
      
```
:::

::: {.cell .code trusted="true"}
``` python
# check again missing values in numerical variables in X_train

X_train[numerical].isnull().sum()
```
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in numerical variables in X_test

X_test[numerical].isnull().sum()
```
:::

::: {.cell .markdown}
Now, we can see that there are no missing values in the numerical
columns of training and test set.
:::

::: {.cell .markdown}
### Engineering missing values in categorical variables
:::

::: {.cell .code trusted="true"}
``` python
# print percentage of missing values in the categorical variables in training set

X_train[categorical].isnull().mean()
```
:::

::: {.cell .code trusted="true"}
``` python
# print categorical variables with missing data

for col in categorical:
    if X_train[col].isnull().mean()>0:
        print(col, (X_train[col].isnull().mean()))
```
:::

::: {.cell .code trusted="true"}
``` python
# impute missing categorical variables with most frequent value

for df2 in [X_train, X_test]:
    df2['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True)
    df2['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True)
    df2['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True)
    df2['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace=True)
```
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in categorical variables in X_train

X_train[categorical].isnull().sum()
```
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in categorical variables in X_test

X_test[categorical].isnull().sum()
```
:::

::: {.cell .markdown}
As a final check, I will check for missing values in X_train and X_test.
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in X_train

X_train.isnull().sum()
```
:::

::: {.cell .code trusted="true"}
``` python
# check missing values in X_test

X_test.isnull().sum()
```
:::

::: {.cell .markdown}
We can see that there are no missing values in X_train and X_test.
:::

::: {.cell .markdown}
### Engineering outliers in numerical variables

We have seen that the `Rainfall`, `Evaporation`, `WindSpeed9am` and
`WindSpeed3pm` columns contain outliers. I will use top-coding approach
to cap maximum values and remove outliers from the above variables.
:::

::: {.cell .code trusted="true"}
``` python
def max_value(df3, variable, top):
    return np.where(df3[variable]>top, top, df3[variable])

for df3 in [X_train, X_test]:
    df3['Rainfall'] = max_value(df3, 'Rainfall', 3.2)
    df3['Evaporation'] = max_value(df3, 'Evaporation', 21.8)
    df3['WindSpeed9am'] = max_value(df3, 'WindSpeed9am', 55)
    df3['WindSpeed3pm'] = max_value(df3, 'WindSpeed3pm', 57)
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.Rainfall.max(), X_test.Rainfall.max()
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.Evaporation.max(), X_test.Evaporation.max()
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.WindSpeed9am.max(), X_test.WindSpeed9am.max()
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.WindSpeed3pm.max(), X_test.WindSpeed3pm.max()
```
:::

::: {.cell .code trusted="true"}
``` python
X_train[numerical].describe()
```
:::

::: {.cell .markdown}
We can now see that the outliers in `Rainfall`, `Evaporation`,
`WindSpeed9am` and `WindSpeed3pm` columns are capped.
:::

::: {.cell .markdown}
### Encode categorical variables
:::

::: {.cell .code trusted="true"}
``` python
categorical
```
:::

::: {.cell .code trusted="true"}
``` python
X_train[categorical].head()
```
:::

::: {.cell .code trusted="true"}
``` python
# encode RainToday variable

import category_encoders as ce

encoder = ce.BinaryEncoder(cols=['RainToday'])

X_train = encoder.fit_transform(X_train)

X_test = encoder.transform(X_test)
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.head()
```
:::

::: {.cell .markdown}
We can see that two additional variables `RainToday_0` and `RainToday_1`
are created from `RainToday` variable.

Now, I will create the `X_train` training set.
:::

::: {.cell .code trusted="true"}
``` python
X_train = pd.concat([X_train[numerical], X_train[['RainToday_0', 'RainToday_1']],
                     pd.get_dummies(X_train.Location), 
                     pd.get_dummies(X_train.WindGustDir),
                     pd.get_dummies(X_train.WindDir9am),
                     pd.get_dummies(X_train.WindDir3pm)], axis=1)
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.head()
```
:::

::: {.cell .markdown}
Similarly, I will create the `X_test` testing set.
:::

::: {.cell .code trusted="true"}
``` python
X_test = pd.concat([X_test[numerical], X_test[['RainToday_0', 'RainToday_1']],
                     pd.get_dummies(X_test.Location), 
                     pd.get_dummies(X_test.WindGustDir),
                     pd.get_dummies(X_test.WindDir9am),
                     pd.get_dummies(X_test.WindDir3pm)], axis=1)
```
:::

::: {.cell .code trusted="true"}
``` python
X_test.head()
```
:::

::: {.cell .markdown}
We now have training and testing set ready for model building. Before
that, we should map all the feature variables onto the same scale. It is
called `feature scaling`. I will do it as follows.
:::

::: {.cell .markdown}
# **11. Feature Scaling** `<a class="anchor" id="11">`{=html}`</a>`{=html} {#11-feature-scaling-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
X_train.describe()
```
:::

::: {.cell .code trusted="true"}
``` python
cols = X_train.columns
```
:::

::: {.cell .code trusted="true"}
``` python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)
```
:::

::: {.cell .code trusted="true"}
``` python
X_train = pd.DataFrame(X_train, columns=[cols])
```
:::

::: {.cell .code trusted="true"}
``` python
X_test = pd.DataFrame(X_test, columns=[cols])
```
:::

::: {.cell .code trusted="true"}
``` python
X_train.describe()
```
:::

::: {.cell .markdown}
We now have `X_train` dataset ready to be fed into the Logistic
Regression classifier. I will do it as follows.
:::

::: {.cell .markdown}
# **12. Model training** `<a class="anchor" id="12">`{=html}`</a>`{=html} {#12-model-training-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
# train a logistic regression model on the training set
from sklearn.linear_model import LogisticRegression


# instantiate the model
logreg = LogisticRegression(solver='liblinear', random_state=0)


# fit the model
logreg.fit(X_train, y_train)
```
:::

::: {.cell .markdown}
# **13. Predict results** `<a class="anchor" id="13">`{=html}`</a>`{=html} {#13-predict-results-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
y_pred_test = logreg.predict(X_test)

y_pred_test
```
:::

::: {.cell .markdown}
### predict_proba method

**predict_proba** method gives the probabilities for the target
variable(0 and 1) in this case, in array form.

`0 is for probability of no rain` and `1 is for probability of rain.`
:::

::: {.cell .code trusted="true"}
``` python
# probability of getting output as 0 - no rain

logreg.predict_proba(X_test)[:,0]
```
:::

::: {.cell .code trusted="true"}
``` python
# probability of getting output as 1 - rain

logreg.predict_proba(X_test)[:,1]
```
:::

::: {.cell .markdown}
# **14. Check accuracy score** `<a class="anchor" id="14">`{=html}`</a>`{=html} {#14-check-accuracy-score-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))
```
:::

::: {.cell .markdown}
Here, **y_test** are the true class labels and **y_pred_test** are the
predicted class labels in the test-set.
:::

::: {.cell .markdown}
### Compare the train-set and test-set accuracy

Now, I will compare the train-set and test-set accuracy to check for
overfitting.
:::

::: {.cell .code trusted="true"}
``` python
y_pred_train = logreg.predict(X_train)

y_pred_train
```
:::

::: {.cell .code trusted="true"}
``` python
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))
```
:::

::: {.cell .markdown}
### Check for overfitting and underfitting
:::

::: {.cell .code trusted="true"}
``` python
# print the scores on training and test set

print('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))
```
:::

::: {.cell .markdown}
The training-set accuracy score is 0.8476 while the test-set accuracy to
be 0.8501. These two values are quite comparable. So, there is no
question of overfitting.
:::

::: {.cell .markdown}
In Logistic Regression, we use default value of C = 1. It provides good
performance with approximately 85% accuracy on both the training and the
test set. But the model performance on both the training and test set
are very comparable. It is likely the case of underfitting.

I will increase C and fit a more flexible model.
:::

::: {.cell .code trusted="true"}
``` python
# fit the Logsitic Regression model with C=100

# instantiate the model
logreg100 = LogisticRegression(C=100, solver='liblinear', random_state=0)


# fit the model
logreg100.fit(X_train, y_train)
```
:::

::: {.cell .code trusted="true"}
``` python
# print the scores on training and test set

print('Training set score: {:.4f}'.format(logreg100.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(logreg100.score(X_test, y_test)))
```
:::

::: {.cell .markdown}
We can see that, C=100 results in higher test set accuracy and also a
slightly increased training set accuracy. So, we can conclude that a
more complex model should perform better.
:::

::: {.cell .markdown}
Now, I will investigate, what happens if we use more regularized model
than the default value of C=1, by setting C=0.01.
:::

::: {.cell .code trusted="true"}
``` python
# fit the Logsitic Regression model with C=001

# instantiate the model
logreg001 = LogisticRegression(C=0.01, solver='liblinear', random_state=0)


# fit the model
logreg001.fit(X_train, y_train)
```
:::

::: {.cell .code trusted="true"}
``` python
# print the scores on training and test set

print('Training set score: {:.4f}'.format(logreg001.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(logreg001.score(X_test, y_test)))
```
:::

::: {.cell .markdown}
So, if we use more regularized model by setting C=0.01, then both the
training and test set accuracy decrease relatiev to the default
parameters.
:::

::: {.cell .markdown}
### Compare model accuracy with null accuracy

So, the model accuracy is 0.8501. But, we cannot say that our model is
very good based on the above accuracy. We must compare it with the
**null accuracy**. Null accuracy is the accuracy that could be achieved
by always predicting the most frequent class.

So, we should first check the class distribution in the test set.
:::

::: {.cell .code trusted="true"}
``` python
# check class distribution in test set

y_test.value_counts()
```
:::

::: {.cell .markdown}
We can see that the occurences of most frequent class is 22067. So, we
can calculate null accuracy by dividing 22067 by total number of
occurences.
:::

::: {.cell .code trusted="true"}
``` python
# check null accuracy score

null_accuracy = (22067/(22067+6372))

print('Null accuracy score: {0:0.4f}'. format(null_accuracy))
```
:::

::: {.cell .markdown}
We can see that our model accuracy score is 0.8501 but null accuracy
score is 0.7759. So, we can conclude that our Logistic Regression model
is doing a very good job in predicting the class labels.
:::

::: {.cell .markdown}
Now, based on the above analysis we can conclude that our classification
model accuracy is very good. Our model is doing a very good job in terms
of predicting the class labels.

But, it does not give the underlying distribution of values. Also, it
does not tell anything about the type of errors our classifer is making.

We have another tool called `Confusion matrix` that comes to our rescue.
:::

::: {.cell .markdown}
# **15. Confusion matrix** `<a class="anchor" id="15">`{=html}`</a>`{=html} {#15-confusion-matrix-}

[Table of Contents](#0.1)

A confusion matrix is a tool for summarizing the performance of a
classification algorithm. A confusion matrix will give us a clear
picture of classification model performance and the types of errors
produced by the model. It gives us a summary of correct and incorrect
predictions broken down by each category. The summary is represented in
a tabular form.

Four types of outcomes are possible while evaluating a classification
model performance. These four outcomes are described below:-

**True Positives (TP)** -- True Positives occur when we predict an
observation belongs to a certain class and the observation actually
belongs to that class.

**True Negatives (TN)** -- True Negatives occur when we predict an
observation does not belong to a certain class and the observation
actually does not belong to that class.

**False Positives (FP)** -- False Positives occur when we predict an
observation belongs to a certain class but the observation actually does
not belong to that class. This type of error is called **Type I error.**

**False Negatives (FN)** -- False Negatives occur when we predict an
observation does not belong to a certain class but the observation
actually belongs to that class. This is a very serious error and it is
called **Type II error.**

These four outcomes are summarized in a confusion matrix given below.
:::

::: {.cell .code trusted="true"}
``` python
# Print the Confusion Matrix and slice it into four pieces

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_test)

print('Confusion matrix\n\n', cm)

print('\nTrue Positives(TP) = ', cm[0,0])

print('\nTrue Negatives(TN) = ', cm[1,1])

print('\nFalse Positives(FP) = ', cm[0,1])

print('\nFalse Negatives(FN) = ', cm[1,0])
```
:::

::: {.cell .markdown}
The confusion matrix shows `20892 + 3285 = 24177 correct predictions`
and `3087 + 1175 = 4262 incorrect predictions`.

In this case, we have

-   `True Positives` (Actual Positive:1 and Predict Positive:1) - 20892

-   `True Negatives` (Actual Negative:0 and Predict Negative:0) - 3285

-   `False Positives` (Actual Negative:0 but Predict Positive:1) - 1175
    `(Type I error)`

-   `False Negatives` (Actual Positive:1 but Predict Negative:0) - 3087
    `(Type II error)`
:::

::: {.cell .code trusted="true"}
``` python
# visualize confusion matrix with seaborn heatmap

cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], 
                                 index=['Predict Positive:1', 'Predict Negative:0'])

sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')
```
:::

::: {.cell .markdown}
# **16. Classification metrices** `<a class="anchor" id="16">`{=html}`</a>`{=html} {#16-classification-metrices-}

[Table of Contents](#0.1)
:::

::: {.cell .markdown}
## Classification Report

**Classification report** is another way to evaluate the classification
model performance. It displays the **precision**, **recall**, **f1** and
**support** scores for the model. I have described these terms in later.

We can print a classification report as follows:-
:::

::: {.cell .code trusted="true"}
``` python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_test))
```
:::

::: {.cell .markdown}
## Classification accuracy
:::

::: {.cell .code trusted="true"}
``` python
TP = cm[0,0]
TN = cm[1,1]
FP = cm[0,1]
FN = cm[1,0]
```
:::

::: {.cell .code trusted="true"}
``` python
# print classification accuracy

classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)

print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))
```
:::

::: {.cell .markdown}
## Classification error
:::

::: {.cell .code trusted="true"}
``` python
# print classification error

classification_error = (FP + FN) / float(TP + TN + FP + FN)

print('Classification error : {0:0.4f}'.format(classification_error))
```
:::

::: {.cell .markdown}
## Precision

**Precision** can be defined as the percentage of correctly predicted
positive outcomes out of all the predicted positive outcomes. It can be
given as the ratio of true positives (TP) to the sum of true and false
positives (TP + FP).

So, **Precision** identifies the proportion of correctly predicted
positive outcome. It is more concerned with the positive class than the
negative class.

Mathematically, precision can be defined as the ratio of
`TP to (TP + FP).`
:::

::: {.cell .code trusted="true"}
``` python
# print precision score

precision = TP / float(TP + FP)


print('Precision : {0:0.4f}'.format(precision))
```
:::

::: {.cell .markdown}
## Recall

Recall can be defined as the percentage of correctly predicted positive
outcomes out of all the actual positive outcomes. It can be given as the
ratio of true positives (TP) to the sum of true positives and false
negatives (TP + FN). **Recall** is also called **Sensitivity**.

**Recall** identifies the proportion of correctly predicted actual
positives.

Mathematically, recall can be given as the ratio of `TP to (TP + FN).`
:::

::: {.cell .code trusted="true"}
``` python
recall = TP / float(TP + FN)

print('Recall or Sensitivity : {0:0.4f}'.format(recall))
```
:::

::: {.cell .markdown}
## True Positive Rate

**True Positive Rate** is synonymous with **Recall**.
:::

::: {.cell .code trusted="true"}
``` python
true_positive_rate = TP / float(TP + FN)


print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))
```
:::

::: {.cell .markdown}
## False Positive Rate
:::

::: {.cell .code trusted="true"}
``` python
false_positive_rate = FP / float(FP + TN)


print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))
```
:::

::: {.cell .markdown}
## Specificity
:::

::: {.cell .code trusted="true"}
``` python
specificity = TN / (TN + FP)

print('Specificity : {0:0.4f}'.format(specificity))
```
:::

::: {.cell .markdown}
## f1-score

**f1-score** is the weighted harmonic mean of precision and recall. The
best possible **f1-score** would be 1.0 and the worst would be 0.0.
**f1-score** is the harmonic mean of precision and recall. So,
**f1-score** is always lower than accuracy measures as they embed
precision and recall into their computation. The weighted average of
`f1-score` should be used to compare classifier models, not global
accuracy.
:::

::: {.cell .markdown}
## Support

**Support** is the actual number of occurrences of the class in our
dataset.
:::

::: {.cell .markdown}
# **17. Adjusting the threshold level** `<a class="anchor" id="17">`{=html}`</a>`{=html} {#17-adjusting-the-threshold-level-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
# print the first 10 predicted probabilities of two classes- 0 and 1

y_pred_prob = logreg.predict_proba(X_test)[0:10]

y_pred_prob
```
:::

::: {.cell .markdown}
### Observations

-   In each row, the numbers sum to 1.

-   There are 2 columns which correspond to 2 classes - 0 and 1.

    -   Class 0 - predicted probability that there is no rain tomorrow.

    -   Class 1 - predicted probability that there is rain tomorrow.

-   Importance of predicted probabilities

    -   We can rank the observations by probability of rain or no rain.

-   predict_proba process

    -   Predicts the probabilities

    -   Choose the class with the highest probability

-   Classification threshold level

    -   There is a classification threshold level of 0.5.

    -   Class 1 - probability of rain is predicted if probability \>
        0.5.

    -   Class 0 - probability of no rain is predicted if probability \<
        0.5.
:::

::: {.cell .code trusted="true"}
``` python
# store the probabilities in dataframe

y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - No rain tomorrow (0)', 'Prob of - Rain tomorrow (1)'])

y_pred_prob_df
```
:::

::: {.cell .code trusted="true"}
``` python
# print the first 10 predicted probabilities for class 1 - Probability of rain

logreg.predict_proba(X_test)[0:10, 1]
```
:::

::: {.cell .code trusted="true"}
``` python
# store the predicted probabilities for class 1 - Probability of rain

y_pred1 = logreg.predict_proba(X_test)[:, 1]
```
:::

::: {.cell .code trusted="true"}
``` python
# plot histogram of predicted probabilities


# adjust the font size 
plt.rcParams['font.size'] = 12


# plot histogram with 10 bins
plt.hist(y_pred1, bins = 10)


# set the title of predicted probabilities
plt.title('Histogram of predicted probabilities of rain')


# set the x-axis limit
plt.xlim(0,1)


# set the title
plt.xlabel('Predicted probabilities of rain')
plt.ylabel('Frequency')
```
:::

::: {.cell .markdown}
### Observations {#observations}

-   We can see that the above histogram is highly positive skewed.

-   The first column tell us that there are approximately 15000
    observations with probability between 0.0 and 0.1.

-   There are small number of observations with probability \> 0.5.

-   So, these small number of observations predict that there will be
    rain tomorrow.

-   Majority of observations predict that there will be no rain
    tomorrow.
:::

::: {.cell .markdown}
### Lower the threshold
:::

::: {.cell .code trusted="true"}
``` python
from sklearn.preprocessing import binarize

for i in range(1,5):
    
    cm1=0
    
    y_pred1 = logreg.predict_proba(X_test)[:,1]
    
    y_pred1 = y_pred1.reshape(-1,1)
    
    y_pred2 = binarize(y_pred1, i/10)
    
    y_pred2 = np.where(y_pred2 == 1, 'Yes', 'No')
    
    cm1 = confusion_matrix(y_test, y_pred2)
        
    print ('With',i/10,'threshold the Confusion Matrix is ','\n\n',cm1,'\n\n',
           
            'with',cm1[0,0]+cm1[1,1],'correct predictions, ', '\n\n', 
           
            cm1[0,1],'Type I errors( False Positives), ','\n\n',
           
            cm1[1,0],'Type II errors( False Negatives), ','\n\n',
           
           'Accuracy score: ', (accuracy_score(y_test, y_pred2)), '\n\n',
           
           'Sensitivity: ',cm1[1,1]/(float(cm1[1,1]+cm1[1,0])), '\n\n',
           
           'Specificity: ',cm1[0,0]/(float(cm1[0,0]+cm1[0,1])),'\n\n',
          
            '====================================================', '\n\n')
```
:::

::: {.cell .markdown}
### Comments

-   In binary problems, the threshold of 0.5 is used by default to
    convert predicted probabilities into class predictions.

-   Threshold can be adjusted to increase sensitivity or specificity.

-   Sensitivity and specificity have an inverse relationship. Increasing
    one would always decrease the other and vice versa.

-   We can see that increasing the threshold level results in increased
    accuracy.

-   Adjusting the threshold level should be one of the last step you do
    in the model-building process.
:::

::: {.cell .markdown}
# **18. ROC - AUC** `<a class="anchor" id="18">`{=html}`</a>`{=html} {#18-roc---auc-}

[Table of Contents](#0.1)

## ROC Curve

Another tool to measure the classification model performance visually is
**ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic
Curve**. An **ROC Curve** is a plot which shows the performance of a
classification model at various classification threshold levels.

The **ROC Curve** plots the **True Positive Rate (TPR)** against the
**False Positive Rate (FPR)** at various threshold levels.

**True Positive Rate (TPR)** is also called **Recall**. It is defined as
the ratio of `TP to (TP + FN).`

**False Positive Rate (FPR)** is defined as the ratio of
`FP to (FP + TN).`

In the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR
(False Positive Rate) of a single point. This will give us the general
performance of the ROC curve which consists of the TPR and FPR at
various threshold levels. So, an ROC Curve plots TPR vs FPR at different
classification threshold levels. If we lower the threshold levels, it
may result in more items being classified as positve. It will increase
both True Positives (TP) and False Positives (FP).
:::

::: {.cell .code}
``` python
# plot ROC Curve

from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = 'Yes')

plt.figure(figsize=(6,4))

plt.plot(fpr, tpr, linewidth=2)

plt.plot([0,1], [0,1], 'k--' )

plt.rcParams['font.size'] = 12

plt.title('ROC curve for RainTomorrow classifier')

plt.xlabel('False Positive Rate (1 - Specificity)')

plt.ylabel('True Positive Rate (Sensitivity)')

plt.show()
```
:::

::: {.cell .markdown}
ROC curve help us to choose a threshold level that balances sensitivity
and specificity for a particular context.
:::

::: {.cell .markdown}
## ROC-AUC

**ROC AUC** stands for **Receiver Operating Characteristic - Area Under
Curve**. It is a technique to compare classifier performance. In this
technique, we measure the `area under the curve (AUC)`. A perfect
classifier will have a ROC AUC equal to 1, whereas a purely random
classifier will have a ROC AUC equal to 0.5.

So, **ROC AUC** is the percentage of the ROC plot that is underneath the
curve.
:::

::: {.cell .code trusted="true"}
``` python
# compute ROC AUC

from sklearn.metrics import roc_auc_score

ROC_AUC = roc_auc_score(y_test, y_pred1)

print('ROC AUC : {:.4f}'.format(ROC_AUC))
```
:::

::: {.cell .markdown}
### Comments {#comments}

-   ROC AUC is a single number summary of classifier performance. The
    higher the value, the better the classifier.

-   ROC AUC of our model approaches towards 1. So, we can conclude that
    our classifier does a good job in predicting whether it will rain
    tomorrow or not.
:::

::: {.cell .code trusted="true"}
``` python
# calculate cross-validated ROC AUC 

from sklearn.model_selection import cross_val_score

Cross_validated_ROC_AUC = cross_val_score(logreg, X_train, y_train, cv=5, scoring='roc_auc').mean()

print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))
```
:::

::: {.cell .markdown}
# **19. k-Fold Cross Validation** `<a class="anchor" id="19">`{=html}`</a>`{=html} {#19-k-fold-cross-validation-}

[Table of Contents](#0.1)
:::

::: {.cell .code trusted="true"}
``` python
# Applying 5-Fold Cross Validation

from sklearn.model_selection import cross_val_score

scores = cross_val_score(logreg, X_train, y_train, cv = 5, scoring='accuracy')

print('Cross-validation scores:{}'.format(scores))
```
:::

::: {.cell .markdown}
We can summarize the cross-validation accuracy by calculating its mean.
:::

::: {.cell .code trusted="true"}
``` python
# compute Average cross-validation score

print('Average cross-validation score: {:.4f}'.format(scores.mean()))
```
:::

::: {.cell .markdown}
Our, original model score is found to be 0.8476. The average
cross-validation score is 0.8474. So, we can conclude that
cross-validation does not result in performance improvement.
:::

::: {.cell .markdown}
# **20. Hyperparameter Optimization using GridSearch CV** `<a class="anchor" id="20">`{=html}`</a>`{=html} {#20-hyperparameter-optimization-using-gridsearch-cv-}

[Table of Contents](#0.1)
:::

::: {.cell .code scrolled="true" trusted="true"}
``` python
from sklearn.model_selection import GridSearchCV


parameters = [{'penalty':['l1','l2']}, 
              {'C':[1, 10, 100, 1000]}]



grid_search = GridSearchCV(estimator = logreg,  
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 5,
                           verbose=0)


grid_search.fit(X_train, y_train)
```
:::

::: {.cell .code trusted="true"}
``` python
# examine the best model

# best score achieved during the GridSearchCV
print('GridSearch CV best score : {:.4f}\n\n'.format(grid_search.best_score_))

# print parameters that give the best results
print('Parameters that give the best results :','\n\n', (grid_search.best_params_))

# print estimator that was chosen by the GridSearch
print('\n\nEstimator that was chosen by the search :','\n\n', (grid_search.best_estimator_))
```
:::

::: {.cell .code trusted="true"}
``` python
# calculate GridSearch CV score on test set

print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))
```
:::

::: {.cell .markdown}
### Comments {#comments}

-   Our original model test accuracy is 0.8501 while GridSearch CV
    accuracy is 0.8507.

-   We can see that GridSearch CV improve the performance for this
    particular model.
:::

::: {.cell .markdown}
# **21. Results and conclusion** `<a class="anchor" id="21">`{=html}`</a>`{=html} {#21-results-and-conclusion-}

[Table of Contents](#0.1)
:::

::: {.cell .markdown}
1.  The logistic regression model accuracy score is 0.8501. So, the
    model does a very good job in predicting whether or not it will rain
    tomorrow in Australia.

2.  Small number of observations predict that there will be rain
    tomorrow. Majority of observations predict that there will be no
    rain tomorrow.

3.  The model shows no signs of overfitting.

4.  Increasing the value of C results in higher test set accuracy and
    also a slightly increased training set accuracy. So, we can conclude
    that a more complex model should perform better.

5.  Increasing the threshold level results in increased accuracy.

6.  ROC AUC of our model approaches towards 1. So, we can conclude that
    our classifier does a good job in predicting whether it will rain
    tomorrow or not.

7.  Our original model accuracy score is 0.8501 whereas accuracy score
    after RFECV is 0.8500. So, we can obtain approximately similar
    accuracy but with reduced set of features.

8.  In the original model, we have FP = 1175 whereas FP1 = 1174. So, we
    get approximately same number of false positives. Also, FN = 3087
    whereas FN1 = 3091. So, we get slighly higher false negatives.

9.  Our, original model score is found to be 0.8476. The average
    cross-validation score is 0.8474. So, we can conclude that
    cross-validation does not result in performance improvement.

10. Our original model test accuracy is 0.8501 while GridSearch CV
    accuracy is 0.8507. We can see that GridSearch CV improve the
    performance for this particular model.
:::

::: {.cell .markdown}
# **22. References** `<a class="anchor" id="22">`{=html}`</a>`{=html} {#22-references-}

[Table of Contents](#0.1)

The work done in this project is inspired from following books and
websites:-

1.  Hands on Machine Learning with Scikit-Learn and Tensorflow by
    Aurélién Géron

2.  Introduction to Machine Learning with Python by Andreas C. Müller
    and Sarah Guido

3.  Udemy course -- Machine Learning -- A Z by Kirill Eremenko and
    Hadelin de Ponteves

4.  Udemy course -- Feature Engineering for Machine Learning by Soledad
    Galli

5.  Udemy course -- Feature Selection for Machine Learning by Soledad
    Galli

6.  <https://en.wikipedia.org/wiki/Logistic_regression>

7.  <https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html>

8.  <https://en.wikipedia.org/wiki/Sigmoid_function>

9.  <https://www.statisticssolutions.com/assumptions-of-logistic-regression/>

10. <https://www.kaggle.com/mnassrib/titanic-logistic-regression-with-python>

11. <https://www.kaggle.com/neisha/heart-disease-prediction-using-logistic-regression>

12. <https://www.ritchieng.com/machine-learning-evaluate-classification-model/>
:::

::: {.cell .markdown}
So, now we will come to the end of this kernel.

I hope you find this kernel useful and enjoyable.

Your comments and feedback are most welcome.

Thank you
:::

::: {.cell .markdown trusted="true"}
[Go to Top](#0)
:::
